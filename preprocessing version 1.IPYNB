{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To begin this exploratory analysis, first import libraries and define functions and utilities to work with the data.\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import numpy as np # linear algebra\n",
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# for beautiful plots and some types of graphs\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING THE DATA\n",
    "# There is 1 csv file in the current version of the dataset\n",
    "\n",
    "kBaseDataDirectory = \"C:\\\\Users\\\\Ayush\\\\OneDrive - BBTech\\\\Desktop\\\\sem 6\\\\BI\\\\datasets\\\\French_fashion_c2c\\\\6M-0K-99K.users.dataset.public.csv\"  # on Kaggle\n",
    "#kBaseDataDirectory = \"./kaggle/input\"  # when working offline with jupyter notebook\n",
    "\n",
    "dataset_files = []\n",
    "\n",
    "# This loop will import all dataset files in case we add more data in a next version of the dataset\n",
    "for dirname, _, filenames in os.walk(kBaseDataDirectory):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        dataset_files.append(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######   Utility functions for statistics   #######\n",
    "\n",
    "### Helpers to filter dataframes\n",
    "\n",
    "def helper_has_fields_compared_to(df, columns, target, what, operator):\n",
    "    \"\"\"\n",
    "    Helper to compare several columns to the same value.\n",
    "    \"\"\"\n",
    "    col = columns[0]\n",
    "    res = df[col] > target\n",
    "    for col in columns[1:]:\n",
    "        if operator == '>':\n",
    "            tmp = (df[col] > target)\n",
    "        elif operator == '>=':\n",
    "            tmp = (df[col] >= target)\n",
    "        elif operator == '<=':\n",
    "            tmp = (df[col] <= target)\n",
    "        elif operator == '<':\n",
    "            tmp = (df[col] < target)\n",
    "        elif operator == '==':\n",
    "            tmp = (df[col] == target)\n",
    "        elif operator == '!=':\n",
    "            tmp = (df[col] != target)\n",
    "        \n",
    "        # \n",
    "        if what == 'all':\n",
    "            res = res & tmp\n",
    "        elif what in ['any']:\n",
    "            res = res | tmp\n",
    "    return res\n",
    "\n",
    "def helper_has_any_field_greater_than(df, columns, target):\n",
    "    \"\"\"Returns lines of the dataframe where any of value of the specified columns\n",
    "    is greater than the target.\n",
    "    \"\"\"\n",
    "    res = helper_has_fields_compared_to(df, columns, target, 'any', '>')\n",
    "    return res\n",
    "\n",
    "def helper_has_all_field_greater_than(df, columns, target):\n",
    "    res = helper_has_fields_compared_to(df, columns, target, 'all', '>')\n",
    "    return res\n",
    "\n",
    "\n",
    "### Other utilities for stats\n",
    "\n",
    "def frequency(data, probabilities=False, sort=False, reverse=False):\n",
    "    \"\"\"Returns the frequency distribution of elements.\n",
    "    This is a convenience method for effectif()'s most common use case, without all the more complicated parameters.\n",
    "    :param data: A collection of elements you want to count.\n",
    "    :param bool probabilities: Whether you want the result frequencies to sum up to 1. Default: False\n",
    "    \"\"\"\n",
    "    xis, nis = effectif(data, returnSplitted=True, frequencies=probabilities, sort=sort, reverse=reverse)\n",
    "    return xis, nis\n",
    "\n",
    "\n",
    "def frequences(data, returnSplitted=True, hashAsString=False, universe=None, frequenciesOverUniverse=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if universe is None:\n",
    "        return effectif(data, returnSplitted, hashAsString, True)\n",
    "    else:\n",
    "        return effectifU(data, universe, returnSplitted, hashAsString, True, frequenciesOverUniverse)\n",
    "    \n",
    "\n",
    "def effectif(data, returnSplitted=True, hashAsString=False, frequencies=False, inputConverter=None, sort=False, reverse=False):\n",
    "    \"\"\"calcule l'effectif\n",
    "    :param list data: une liste\n",
    "    :param bool hashAsString: whether we should convert the values in 'data' to\n",
    "                string before comparing them\n",
    "    :param function inputConverter: a callable function that is used to convert\n",
    "                the values within data into the class you want the values to be\n",
    "                compared as. When not provided, the identity function is used.\n",
    "                If used with parameter 'hashAsString', the hashed value will be\n",
    "                the one returned by this function.\n",
    "    :param bool sort: sort the result (only if returnSplitted). Shorthand for `sortBasedOn`\n",
    "    :param bool reverse: reverse the order (only if sort and returnSplitted). Shorthand for `sortBasedOn`\n",
    "    \"\"\"\n",
    "    inputConverter = (lambda x: x) if inputConverter is None else inputConverter\n",
    "    effs = {}\n",
    "    for val in data:\n",
    "        val = inputConverter(val)\n",
    "        key = str(val) if hashAsString else val\n",
    "        try:\n",
    "            effs[key] = effs[key]+1\n",
    "        except:\n",
    "            effs[key] = 1\n",
    "    \n",
    "    if frequencies:\n",
    "        tot = sum(effs.values())\n",
    "        for key in effs:\n",
    "            effs[key] = effs[key]/tot\n",
    "    \n",
    "    if returnSplitted:\n",
    "        xis = list(effs.keys())\n",
    "        nis = list(effs.values())\n",
    "        if sort:\n",
    "            xis, nis = sortBasedOn(nis, xis, nis, reverse=reverse)\n",
    "        return xis, nis\n",
    "    \n",
    "    return effs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution graphs (histogram/bar graph) of column data\n",
    "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
    "    nunique = df.nunique()\n",
    "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
    "    nRow, nCol = df.shape\n",
    "    columnNames = list(df)\n",
    "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
    "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
    "    for i in range(min(nCol, nGraphShown)):\n",
    "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
    "        columnDf = df.iloc[:, i]\n",
    "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
    "            valueCounts = columnDf.value_counts()\n",
    "            valueCounts.plot.bar()\n",
    "        else:\n",
    "            columnDf.hist()\n",
    "        plt.ylabel('counts')\n",
    "        plt.xticks(rotation = 90)\n",
    "        plt.title(f'{columnNames[i]} (column {i})')\n",
    "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
    "    plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "def plotCorrelationMatrix(df, graphWidth, segmentName=None):\n",
    "    filename = segmentName if segmentName else getattr(df, \"dataframeName\", segmentName)\n",
    "    df = df.dropna('columns') # drop columns with NaN\n",
    "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
    "    if df.shape[1] < 2:\n",
    "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
    "        return\n",
    "    corr = df.corr()\n",
    "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
    "    corrMat = plt.matshow(corr, fignum = 1)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    plt.gca().xaxis.tick_bottom()\n",
    "    plt.colorbar(corrMat)\n",
    "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
    "    plt.show()\n",
    "\n",
    "# Scatter and density plots\n",
    "def plotScatterMatrix(df, plotSize, textSize):\n",
    "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
    "    # Remove rows and columns that would lead to df being singular\n",
    "    df = df.dropna('columns')\n",
    "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
    "    columnNames = list(df)\n",
    "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
    "        columnNames = columnNames[:10]\n",
    "    df = df[columnNames]\n",
    "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
    "    corrs = df.corr().values\n",
    "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
    "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
    "    plt.suptitle('Scatter and Density Plot')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
